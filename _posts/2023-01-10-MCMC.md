---
title: Bayesian inference and MCMC
date: 2023-01-10 00:00:01 
categories: [Tutorial, Bayesian estimation]
tags: [bayes, tutorial, math, estimation, monte-carlo]     # TAG names should always be lowercase
math : true
toc : true
---

This tutorial presents some elements on bayesian inference and the method of Monte-Carlo with Marlov Chains. It should be useful only as an introduction to those methods as I am not an expert in this field. 


Some basics to have to follow this tutorial :
- Basic python skills
- Probabilities : density function
- Other maths : integrals


```python
# Some libraries 

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import uniform, expon, norm, multivariate_normal, truncnorm
import time
from markov import MarkovChain 
import seaborn as sns
import pandas as pd
```

# A. Introduction to Bayesian inference

Inspired by the course of M.L. Delignette-Muller "Introduction to Bayesian Inference" for VetAgroSup [^vetagro].

## 1. Bayesian inference  vs Frequentist inference
Bayesian inference and frequentist inference are methods of statistical inference that are consequences of two different paradigms: frequentism and bayesianism.

Different views of what is a probability :
- In a frequentist perspective, the probability of an event is defined as the fraction of times that the event occurs in a very large number of trials.
- In a bayesian prespective, the probability is seen as a degree of belief, a measure of uncertainty.

Inference refers to the fit of the model to the data. The model refers to a process that generates observed data Y. It could depend on known covariates X. For example the most simple model will be Y=X. But usually a model will have a stochastic part as well as unknown parameters $\theta$. The goal of inference will be to estimate $\theta$.

- For frequentist inference : the parameter $\theta$ is assumed fixed but unknown. It is estimated using methods like :
    - Moment matching
    - Maximum likelihood
    - Sum of squared deviations minimization 
- Bayesian inference : the parameter $\theta$ is supposed uncertain and its uncertainty is characerized by a probability distribution (a degree of belief). This distribution is estimated using Bayes theorem.

### Frequentist inference : confidence interval and p-value

Three results are of interest for frequentist inference :
- The point estimate
- The confidence interval
- Hypothesis test with p-value

The **confidence interval** is based on imagining repeated sampling from the model. If we repeatedly obtained samples of size n from the population and constructed a 95% confidence interval for each, we could expect 95% of the intervals to contain the true value of the parameter. Note that it does not mean that it does not mean that the parameter has a 95% chance of being in the interval as precisely in the frequentist framework, such probability does not exists. **p-value** will not be discussed here, but the interpretation of p-values must be very careful and the intepretation is not straightforward. 

### Bayesian inference : distribution and credible interval

The Bayesian inference obtains a posterior distribution of $\theta$.

To obtain a point estimate of the parameters, we would consider the mean or the median for example. Credible intervals (sometimes loosly refered to as confidence interval) are drawn from the posterior distribution quantiles. It is easier to interpret than a confidence interval. 

There is no need to have an equivalent of hypothesis testing. The distributions carries all the information we could need about the uncertainty of our estimation.


## 2. Bayesian inference in practice

First we need a prior distributions P($\theta$). Let's note here that P will be a density in our case. If you have no information about the prior, distributions like the uniform distributions are considered uninformative. Then you can apply the Bayes formula to obtain the posterior distribution :

$$P(\theta \mid Y)=\frac{P(Y \mid \theta) \times P(\theta)}{P(Y)} \propto P(Y \mid \theta) \times P(\theta)$$

An analytical result only exists in some cases. It is the reason why it is hard for most cases to apply Bayesian inference. MCMC allows to draw samples from complex analytical expressions. 

- $P(Y \mid \theta)$ is the likelihood
- $P(\theta)$ is the prior
- $P(Y)$ is the evidence but it is not necessary to compute it since it is a constant.
- $P(\theta \mid Y)$ is the posterior distribution we want to obtain

An interesting mecanism of bayesian inference is that if we have the data Y = [y1, ..., yn] and if we alread have the posterior $P(\theta \mid [y2, ...,yn])$ : 
$$P(\theta \mid Y) \propto P(y1 \mid \theta) \times P(\theta \mid [y2, ...,yn])$$

This formula means that we can iterate by considering our posterior as our new prior. 

Another way to see it is that the likelihood is : $P(Y \mid \theta)=\prod_i P(yi \mid \theta)$

### Example of bayesian inference

This example is copied from [source](https://datapythonista.me/blog/bayesian-inference-tutorial-a-hello-world-example.html) [^datapythonista].

The problem is to approximate the mean and the variance of a normal law that represent the distribution of heights in a human population like say for example pyton programmers. We have the data points : [183, 168, 177, 170, 175, 177, 178, 166, 174, 178].

In this case, we have directly the likelihood for one measure. It is our model.

$$P(x \mid \mu, \sigma)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} \cdot e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}$$

An example of prior could be based on the world population :
- uniform [0,300] for mean
- uniform [2,20] for std

We can obtain a first posterior evaluation for a theta with : $$P(\mu, \sigma \mid x1) \propto P(x1 \mid \mu, \sigma)P(\mu, \sigma)$$

Then we iterate to obtain : $$P(\mu, \sigma \mid x1,x2) \propto P(x2 \mid \mu, \sigma)P(\mu, \sigma \mid x1)$$

We repeat this n times to only obtain one value of the posterior. As said before the other way to see this is that we compute the total likelihood like the product of the marginal likelihoods.

>In general we can sum up the calculus of the posterior like this :
>
>$$P(\theta \mid Y) \propto P(Y \mid \theta) \times P(\theta) = \prod_i P(yi \mid \theta)\times P(\theta)$$

In our case the expression of the likelihood is straightforward but it is not always the case. 

```python
data_h = np.array([183, 168, 177, 170, 175, 177, 178, 166, 174, 178])

def get_posterior(theta, data_h, prior, likelihood):
    posterior = prior(theta)
    likelihoods = likelihood(data_h, theta)
    for i in range(len(data_h)):
        posterior = likelihoods[i]*posterior
    return posterior

def compute_posteriors(mu_candidates, sigma_candidates, data_h, prior, likelihood):
    for mu in mu_candidates:
        for sigma in sigma_candidates:
            yield mu, sigma, get_posterior([mu, sigma], data_h, prior, likelihood)

def normalize(res):
    res['posterior'] = res['posterior']/res['posterior'].sum()
    return res
    

def get_heatmap(res):
    results = pd.DataFrame(res, columns=('mu', 'sigma', 'posterior')).set_index(['mu', 'sigma'])
    res = np.array(res)
    mu = res[:,0]
    sigma = res[:,1]
    posterior = res[:,2]
    mu_mean, sigma_mean = np.average(mu,weights=posterior),np.average(sigma,weights=posterior)
    results = normalize(results)['posterior']
    mu_max,sigma_max = results.idxmax()
    max_posterior = results[(mu_max,sigma_max)]
    print('mu(max_posterior) : {}'.format(mu_max))
    print('sigma(max_posterior) : {}'.format(sigma_max))
    print('mu_mean = {}'.format(mu_mean))
    print('sigma_mean = {}'.format(sigma_mean))
    print('max_posterior : {}'.format(max_posterior))
    heatmap_results = results.unstack(-1)
    heatmap_results.index = heatmap_results.index.astype(int)
    heatmap_results = heatmap_results.T
    heatmap_results.index = heatmap_results.index.astype(int)
    return heatmap_results
```

Here are three choices of priors. prior1 and prior1ter are good choices but prior1bis is a bad choice because we give too much confidence in probably false information (the prior for the mean indicates that it almost sure that the mean is 165). 

```python
def prior1(theta):
    return uniform.pdf(theta[0],loc=100,scale=100)*uniform.pdf(theta[1],loc=2,scale=12)

def prior1bis(theta):
    return norm.pdf(theta[0],loc=165,scale=1)*uniform.pdf(theta[1],loc=2,scale=12)

def prior1ter(theta):
    return norm.pdf(theta[0],loc=165,scale=14)*uniform.pdf(theta[1],loc=2,scale=12)

def likelihood1(h, theta):
    return norm.pdf(h,loc=theta[0],scale=theta[1])
```

```python
get_posterior([165,8], data_h,prior1, likelihood1)
```
    8.767348018647538e-21



Our strategy here will be to compute the value of the posteriori function on a grid of values of the parameters. An issue of this method is we don't really have any mean to know how to define this grid.  The size of the grid can be too much to compute. 

A logical choice for the grid could be to make it correspond our prior. In the case of a finite support for the prior density (like a uniform density), it is straightforward. In the other case (infinite support), the representation of the density will be truncated by the choice of our grid. 

In our case we will choose a grid that corresponds to out first prior prior1.

```python
mu_candidates = np.linspace(100,200,200)
sigma_candidates = np.linspace(2,14,200)
t = time.time()
results1 = list(compute_posteriors(mu_candidates, sigma_candidates, data_h, prior1, likelihood1))
print(time.time()-t)
h1 = get_heatmap(results1)
sns.heatmap(h1,xticklabels=18,yticklabels=40,cbar=False)
plt.show()
```
    6.284130334854126
    mu(max_posterior) : 174.3718592964824
    sigma(max_posterior) : 4.954773869346734
    mu_mean = 174.5999999999147
    sigma_mean = 6.084374303458606
    max_posterior : 0.002180681040617771

![png]({{ site.baseurl }}/assets/20230110/MCMC/MCMC_14_1.png){: .center-image }

We can see a bad prior influence on the result for results2 (with prior1bis). Here our normal prior for the mean is very informative (with a variance of 1 for the prior normal law for the mean). In this case do not confuse the variance of the mean's prior normal law and the variance of the normal law describing the distribution of heights.

```python
t = time.time()
results2 = list(compute_posteriors(mu_candidates, sigma_candidates, data_h, prior1bis, likelihood1))
print(time.time()-t)
h2 = get_heatmap(results2)
sns.heatmap(h2,xticklabels=18,yticklabels=40,cbar=False)
plt.show()
```
    6.401662349700928
    mu(max_posterior) : 165.8291457286432
    sigma(max_posterior) : 10.080402010050252
    mu_mean = 165.88015342638164
    sigma_mean = 10.420187716145758
    max_posterior : 0.0023535275079086372

![png]({{ site.baseurl }}/assets/20230110/MCMC/MCMC_16_1.png){: .center-image }

Here is a less informative prior but that does not conveys false information (but it is still nonetheless more informative than the prior1). 

```python
t = time.time()
results3= list(compute_posteriors(mu_candidates, sigma_candidates, data_h, prior1ter, likelihood1))
print(time.time()-t)
h3= get_heatmap(results3)
sns.heatmap(h3,xticklabels=18,yticklabels=40,cbar=False)
plt.show()
```
    6.611626386642456
    mu(max_posterior) : 174.3718592964824
    sigma(max_posterior) : 4.954773869346734
    mu_mean = 174.4103834766401
    sigma_mean = 6.078988272137783
    max_posterior : 0.002216768096647653

![png]({{ site.baseurl }}/assets/20230110/MCMC/MCMC_18_1.png){: .center-image }

In general bayesian inference efficiency will depend on :
- the choice of the prior : subjective choice
- the complexity to compute the posteriori : depends on the size of the data and the complexity of the likelihood function

The efficiency of the grid method can be judged through :
- the resolution of our grid : it means a more precise representation of the posteriori density but more calculus.
- the extension of the grid : the choice is as subjective as the choice of the prior, to be careful you have to choose a large grid but it also means that you will need a greater resolution to capture the form of the density (so more calculus).

In our case : we had to calculate $200*200=40000$ posteriors. Let's note that a solution to avoid the problems of the grid methods (choice of grid and resolution), an alternative is to sample from the prior distribution. The indicated method for this MCMC, we will see why in the following part. 

### Comparison with frequentist inference

We are going to maximize the likelihood. We can very simply derive the maximum likelihood from our bayesian inference with a uniform prior. In deed, in the case of uniform prior, the maximum a posteriori is equal to maximum likelihood estimator. 

$$\text{max}_{\theta}(P(Y \mid \theta))=\text{max}_{\theta}(P(\theta \mid Y))$$ 

Let's note that there are more rapid means to obtain the maximum likelihood by doing a gradient descent. So frequentist will be more rapid than bayesian inference. 

#### Credible vs Confidence interval

This will be maybe the subject of a future post ! 

## 3. Conclusion

> Important points : 
> - Bayesian posteriors have complex expressions and it hard to know on what domain and in which resolution to compute it : Sampling is the solution we will describe after with MCMC. 
> - Frequentist and bayesian inference differences : bayesian inference yields a distribution which is versatile and easy to interpret and frequentist yields a point estimate + confidence intervals and hypothesis testing. 
> - Frequentist and bayesian inference similitudes : in the case of a uniform prior in bayesian MAP (maximum a posteriori) and MLE (maximum likelihood estimator) are equal.
{: .prompt-tip }


Why we need MCMC ? With the 2D $\theta$ considered here, the times of calculus are not yet prohibitive. But imagine, we have 10 dimensions and we want a resolution of 200 in each dimensions, we will have $200^{8}$ times more important calculus time. (We have here 10 seconds of calculus, so with a 10 dimensions $\theta$, it will be approximately 8 trillions of centuries to finish our calculus).

# B. Understanding the Metropolis-Hastings Algorithm

Based on the paper by Chib and Greenberg [^chib-greenberg]

## 1. Definition of a probability density

According to [Wikipedia](https://en.wikipedia.org/wiki/Probability_density_function) [^wikidensity]

A probability density function is most commonly associated with absolutely continuous univariate distributions. A random variable X has density $f_X$, where $f_X$ is a non-negative Lebesgue-integrable function, if:
$\operatorname{Pr}[a \leq X \leq b]=\int_{a}^{b} f_{X}(x) d x$

## 2. Acceptance-rejection sampling
The objective is to generate samples from the absolutely continuous target density : $\pi(x)=f(x)/K$. Where f is an unnormalized density and K an unknown constant. 
The only thing needed is knowing a density h(x) that can be simulated such that it exists c so that $f(x)<=ch(x)$. Then the algorithm is the following. It is not trivial to obtain the domination condition !

```python
def ar_sample(f,h,sample_h,c):
    z = sample_h()
    u = uniform.rvs()
    if u<=f(z)/(c*h(z)):
        return z
    else:
        return ar_sample(f,h,sample_h,c)
        
def ar_samples(f,h,sample_h,c,n):
    samples = []
    for i in range(n):
        samples += [ar_sample(f,h,sample_h,c)]
    return samples
```

An important implication of the dominance condition is that f and h should be defined on the same domains. For finite domain functions, it is easy to find a working h function (we know how to sample from a uniform distribution for example).

### Generate samples without A/R

Before using A/R, we saw that we had to know how to sample from a known distribution that is dominant.  We are going to assume here that we know how to sample from a uniform distribution and an exponential distribution. 

### Simulation of a finite defintion domain density knowing how to sample from a uniform density : triangle distribution
We are going to sample from a triangle distribution assuming we know how to sample from a uniform distribution with scipy.stats.uniform.

```python
def custom_triangular(x,a,b,m):
    if a<m<b:
        if x<a or x>b:
            return 0
        elif x<=m:
            return (x-a)/(m-a)
        else:
            return (x-b)/(m-b)
    else:
        raise Exception('Not a<m<b !')
```

In this example the distribution will be normalized. It allows us to check if the sampling is correct with a density plot. For unnormalized functions, some functions canbe normalized by explicitly calculating the integral or by calclating the integral with Monte-Carlwhich is quite simple. 

```python
t1 = lambda x : custom_triangular(x,a=0,b=2,m=1)
```

```python
x = np.linspace(0,2,100)
plt.plot(x, [t1(xi) for xi in x])
```

![png]({{ site.baseurl }}/assets/20230110/MCMC/MCMC_35_1.png){: .center-image }

```python
h1 = lambda x : uniform.pdf(x, loc=0, scale=2)
sample_h1 = lambda : uniform.rvs(loc=0, scale=2)
c1 = 2
s = ar_samples(t1, h1, sample_h1,c1,10000)
```

```python
plt.hist(s, bins=50, density=True)
plt.plot(x, [t1(xi) for xi in x])
```

![png]({{ site.baseurl }}/assets/20230110/MCMC/MCMC_37_1.png){: .center-image }

The choice of c can be important. If c doesn't respect the condition of dominance, the sampling is wrong. And if c is much larger that it could be, there will be more rejections and thus the algorithm will generate the the sample more slowly. 

```python
c1 = 1
t = time.time()
s = ar_samples(t1, h1, sample_h1,c1,10000)
plt.hist(s, bins=50, density=True)
plt.plot(x, [t1(xi) for xi in x])
```

![png]({{ site.baseurl }}/assets/20230110/MCMC/MCMC_39_1.png){: .center-image }

```python
c1 = 2
t = time.time()
s = ar_samples(t1, h1, sample_h1,c1,10000)
print('Time : {}'.format(time.time()-t))
c1 = 4
t = time.time()
s = ar_samples(t1, h1, sample_h1,c1,10000)
print('Time : {}'.format(time.time()-t))
```
    Time : 2.172959566116333
    Time : 4.296980857849121

The optimal choice for c is $\sup _{x} \frac{f(x)}{h(x)}$.

### Simulation of a non-finite defintion domain density knowing how to sample from an exponential : normal distribution
We are going to sample from a N(0,1) assuming we know how to sample from an exponential distribution with density $e^{-x}$ with scipy.stats.expon. To obtain any normal distribution we can simply multiply the samples to multiply the variance and add a constant to the sample to add it to the mean. 
By symmetry of the normal density, to generate samples we can generate a sample from :
$f(x)=\frac{2}{\sqrt{2 \pi}} e^{-x^{2} / 2}$ and then set the sign to - with probability 1/2.

For c we have $g(x)=f(x)/f_{exp}(x)=e^{x-x^{2} / 2} \sqrt{2 / \pi}$. By finding the zero of the derivate of g, we have the maximum of g for x=1. So we have $c=g(1)=\sqrt{2 e / \pi}$

```python
const = np.sqrt(2*np.pi)
n1 = lambda x : np.exp((-x**2)/2)*2/const
```

```python
x = np.linspace(0,5,100)
plt.plot(x, [n1(xi) for xi in x])
```

![png]({{ site.baseurl }}/assets/20230110/MCMC/MCMC_44_1.png){: .center-image }

```python
h2 = lambda x : expon.pdf(x)
sample_h2 = lambda : expon.rvs()
c2 = np.sqrt(2*np.e/np.pi)
s = ar_samples(n1, h2, sample_h2,c2,10000)
```

```python
plt.hist(s, bins=50, density=True)
plt.plot(x, [n1(xi) for xi in x])
```

![png]({{ site.baseurl }}/assets/20230110/MCMC/MCMC_46_1.png){: .center-image }

To obtain the complete sample from N(a,b)

```python
def ar_sample_norm(a,b):
    s = ar_sample(n1, h2, sample_h2,c2)
    if uniform.rvs()<1/2:
        return -b*el+a
    else:
        return b*el+a 

def ar_samples_norm(a,b,n):
    s = ar_samples(n1, h2, sample_h2,c2,n)
    return [-b*el+a if uniform.rvs()<1/2 else b*el+a for el in s]
```

```python
t = time.time()
s = ar_samples_norm(2,1,10000)
time.time()-t
```

    1.5570766925811768



```python
plt.hist(s, bins=50, density=True)
plt.plot(x, [norm.pdf(xi, loc=2, scale=1) for xi in x])
```

![png]({{ site.baseurl }}/assets/20230110/MCMC/MCMC_50_1.png){: .center-image }

>**A-R allows to sample from a density but it requires to know how to sample from another dominating density. Obtaining the dominance is not always easy as we will see.**

## 3. Markov Chain Monte-Carlo : principle

### Markov chain

A Markov chain or Markov process is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. (Source : [Wikipedia](https://en.wikipedia.org/wiki/Markov_chain) [^wikimarkov]). Usually Markov chain refers to discrete time Markov chains with a discrete state space. A discrete time discrete state-space markov chain in determined by a stochastic matrix. For example we could imagine a dumb meteo model with a Markov chain model. Imagine there is a transition every day. Each transition has the probability. A remark is that the sum of transitions departing from one state is equal to one. 

```python
# "Dumb" Markov Chain for the weather

P = np.array([
    [0.5, 0.4, 0.1],
    [0.5, 0.3, 0.2],
    [0.3, 0.6, 0.1],
])
mc = MarkovChain(P, ['Sunny', 'Rainy', 'Stormy']) # Thanks to Naysan Saran : https://github.com/NaysanSaran/markov-chain
mc.draw()
```

![png]({{ site.baseurl }}/assets/20230110/MCMC/MCMC_55_0.png){: .center-image }

There has been a great deal of research concerning Markov Chains (discrete and other types) with a lot of interesting properties. We are not going here to write book of maths but rather try to develop an intuition on Markov Chain to use it for MCMC.

### Continuous state-space Markov chains

It is a bit more complicated case than the traditional discrete markov chain. The main difference is that the set of states is continuous. For example, in our last example we could go to any state between rainy and sunny. Let's note here that the time is still discrete ! It is only the state-space that has become continuous. One you understand this notations, you can have a sense of what is going on it a continuous state-space chain :
- Instead of probabilities transition, we have a one step transition density function $p(x,y)$. For example the probability to transit to a state x to somewhere in a set A is $P(x,A)=\int_{\mathcal{A}}P(x, d y) dy$
- We can define n-step transition densities (S is the entire space considered) : $p^{(n)}(x, y)=\int_{\mathcal{S}} p^{(m)}(x, z) \cdot p^{(n-m)}(z, y) d z$ with $p^{(2)}(x, y)=\int_{\mathcal{S}} p(x, z) \cdot p(z, y) d z$ and $p^{(1)}(x, y)=p(x,y)$. This formula gives the density when you go through n steps of the Markov chain. So the probability to go from x to somewhere in A after n steps of the Markov chain is simply : $P^{(n)}(x,A)=\int_{\mathcal{A}}P^{(n)}(x, d y) dy$

As an example of a realization. Let's consider a the jumps of a frog. We can model that the length of the jumps are following a normal law. Then we have : $p(x, y)=\frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2 \sigma^{2}}(y-x)^{2}}$.  

```python
def frog(initial_state, mu, sigma, n):
    state = initial_state
    realization = [initial_state]
    for i in range(n):
        jump = norm.rvs(scale=sigma, loc=mu)
        state += jump
        realization += [state]
    return realization

jumps = frog(0,0,1,2000)
plt.plot(jumps)
```

![png]({{ site.baseurl }}/assets/20230110/MCMC/MCMC_59_1.png){: .center-image }

>**A markov chain is a stochastic process with description of the probability of transitions between states.**

### Convergence to invariant distributions
#### Irreducible and aperiodic chains means convergence to a stationary distribution

We are going to to consider the case of continuous state-space Markov chains in MCMC but our examples in this part will in the discrete case to illustrate.

A Markov chain is defined by P(x,A) a transition kernel. It is the probability of moving from a point x to a point in the set A. For example if we are in R, P(x,R)=1 because there is a probability of 1 to move from x to another point in R. It is possible to move from x to {x}. 

It could be interesting to find an invariant distribution. An invariant distribution satisfies : $\pi(y)=\int_{\mathcal{R}^{d}} P(x, y) \pi(x) d x$. It can be shown that under some conditions, if we iterate the Markov Chain, it converges to the invariant distribution .  It is easier to understand this definition in its discrete version with a transition Matrix M. We have $\pi$ the probabilities to transition to each state is invariant if $\pi=\pi M$. The condition to be sure that the chain will converge towards its stationnary distribution is that it is aperiodic and irreducible (from one state, you can go to any other state in a finite set of move). We have exactly the same conditions for the convergence to a stationary distribution in the continuous case.

For example in the discrete case with a Markov chain with a transition Matrix : 

$$
M_0=\left(\begin{array}{ccccc}
\frac{1}{2} & 0 & 0 & 0 & \frac{1}{2} \\
0 & \frac{1}{2} & 0 & \frac{1}{2} & 0 \\
0 & 0 & 1 & 0 & 0 \\
0 & \frac{1}{4} & \frac{1}{4} & \frac{1}{4} & \frac{1}{4} \\
\frac{1}{2} & 0 & 0 & 0 & \frac{1}{2}
\end{array}\right)
$$

It can be proven solving the condition of invariance that : $\pi=(a, 0, b, 0, a)$ with $2*a+b=1$.
So we can deduce that the Markov chain will converge to a distribution of that type. If we look closely the Markov chain is stucked at state 0 and 4 or at state 2. So the two only possible converging invariant distribution are (1/2,0,0,0,1/2) and (0,0,1,0,0). It depends on the value of the first element. We have this kind of behavior because this finite Markov chain is reducible. 

Let's consider the following transition Matrix when it is aperiodic and irreducible. Basically, it means that all states are connected (the chain never remains stucked)

$$
M_1=\left(\begin{array}{ccccc}
\frac{1}{2} & 0 & \frac{1}{4} & 0 & \frac{1}{4} \\
0 & \frac{1}{2} & 0 & \frac{1}{2} & 0 \\
0 & 0 & \frac{3}{4} & 0 & \frac{1}{4} \\
0 & \frac{1}{4} & \frac{1}{4} & \frac{1}{4} & \frac{1}{4} \\
\frac{1}{4} & 0 & \frac{1}{4} & 0 & \frac{1}{2}
\end{array}\right)
$$

In this case, we observe that the chain converges each time to same stationnary distribution. We can see that because if we plot the empirical frequencies (histogram) we obtain almost the same probabilities for each state, it converges the same way for any starting state.

```python
def iterate_markov(M, state):
    elements = list(range(len(M)))
    transitions = M[state]
    return np.random.choice(elements, 1, p=transitions)[0]

def n_iterate_markov(M, initial_state, n):
    state = initial_state
    states = [initial_state]
    for i in range(n):
        state = iterate_markov(M, state)
        states += [state]
    return states
```

```python
M1 = np.array([[1/2,0,1/4,0,1/4],[0,1/2,0,1/2,0],[0,0,3/4,0,1/4],[0,1/4,1/4,1/4,1/4],[1/4,0,1/4,0,1/2]])
initial_state = 3
s_markov1 = n_iterate_markov(M1, initial_state, 50000)
plt.hist(s_markov1)
```
![png]({{ site.baseurl }}/assets/20230110/MCMC/MCMC_66_1.png){: .center-image }

```python
def get_proba(s,M):
    size = len(M)
    tot = len(s)
    res = [0]*size
    for el in s:
        res[el] += 1
    for i in range(size):
        res[i] = res[i]/tot
    return res
```

```python
pi1 = get_proba(s_markov1, M1)
pi1
```
    [0.16515669686606269,
     0.0,
     0.500689986200276,
     5.999880002399952e-05,
     0.33409331813363735]

> **To sum up : If there is an invariant distribution. And if the chain is irreducible and aperiodic : it will converge to that invariant distribution.**

#### Reversibility implies invariance

Another notion is important. A distribution could be reversible for a Markov chain. It means :
- $\pi(i)P_{i,j}=\pi(j)P_{j,i}$ in the discrete case
- $\pi(x)p(x,y)=\pi(y)p(y,x)$ in the continuous case
The reversibility implies the invariance. 

Let's verify the reversibility for M1. 

```python
def verify_reversibility(M, pi):
    l = len(M)
    res = M.copy()
    for i in range(l):
        for j in range(l):
            res[i,j] = pi[i]*M[i,j]-pi[j]*M[j,i]
    return res
```

```python
res1 = verify_reversibility(M1, pi1)
res1
```
    array([[ 0.00000000e+00,  0.00000000e+00,  4.12891742e-02,
             0.00000000e+00, -4.22341553e-02],
           [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
            -1.49997000e-05,  0.00000000e+00],
           [-4.12891742e-02,  0.00000000e+00,  0.00000000e+00,
            -1.49997000e-05,  4.16491670e-02],
           [ 0.00000000e+00,  1.49997000e-05,  1.49997000e-05,
             0.00000000e+00,  1.49997000e-05],
           [ 4.22341553e-02,  0.00000000e+00, -4.16491670e-02,
            -1.49997000e-05,  0.00000000e+00]])



We can see that in this case, the Markov chain has a distribution that is stationary but not reversible. Let's consider :

$$
M_2=\left(\begin{array}{cccc}
0 & \frac{1}{2} & \frac{1}{2} & 0  \\
\frac{1}{2} & 0 & \frac{1}{2} & 0 \\
\frac{1}{3} & \frac{1}{3} & 0 &\frac{1}{3}  \\
0 & 0 & 1 & 0 \\
\end{array}\right)
$$

It is reversible and thus stationnary.

```python
M2 = np.array([[0,1/2,1/2,0],[1/2,0,1/2,0],[1/3,1/3,0,1/3],[0,0,1,0]])
initial_state = 2
s_markov2 = n_iterate_markov(M2, initial_state, 50000)
plt.hist(s_markov2)
```

![png]({{ site.baseurl }}/assets/20230110/MCMC/MCMC_75_1.png){: .center-image }

```python
pi2 = get_proba(s_markov2, M2)
pi2
```
    [0.249115017699646,
     0.24697506049879003,
     0.376052478950421,
     0.12785744285114298]

```python
pi2 = [1/4,1/4,3/8,1/8] # we round up the values of pi2
res2 = verify_reversibility(M2, pi2)
res2
```
    array([[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]])

>**Reversibility implies invariance.**

### Markov chain Monte-Carlo with Metropolis-Hastings

The idea of MCMC is to use a kernel function p(x,y) such that it is reversible with the $\pi$ we want to simulate.Then is will converge to $\pi$. It inspired of the A/R method because :
- you choose a transition kernel
- at each transition you accept or reject the transition with a certain probability to verifiy th e reversibility condition.

Say that we want to draw samples from $\pi(x)$.
Given a candidate transition kernel q(x,y), we will have either $\pi(x)q(x,y)>\pi(y)q(y,x)$ or $\pi(x)q(x,y)<\pi(y)q(y,x)$. The majority of the time the reversibility is not verified. But we can correct that by computing each time a probability $\alpha(x,y)$ to stay at the same state. So in practice we are going to use the kernel density : p(x,y) = q(x,y)$\alpha(x,y)$.

The question is : how to obtain  $\pi(x)q(x,y)\alpha(x,y)=\pi(y)q(y,x)\alpha(y,x)$ ?

Let's consider the two cases :
- We have $\pi(x)q(x,y)>\pi(y)q(y,x)$. Our goal is to make the probability to transition smaller. So we can choose $\alpha(y,x)=1$ and $\alpha(x,y)=\pi(y)q(y,x)/\pi(x)q(x,y)$.
- We have $\pi(x)q(x,y)<\pi(y)q(y,x)$. Our goal is to make the probability to transition bigger. So we can choose $\alpha(x,y)=1$. Since $\alpha(y,x)$ will be computed in the same way, we will have the reversibility. 

We can resume the choise of $\alpha(x,y)$ by :

$$
\begin{aligned}
\alpha(x, y) &=\min \left[\frac{\pi(y) q(y, x)}{\pi(x) q(x, y)}, 1\right], & & \text { if } \pi(x) q(x, y)>0 \\
&=1, & & \text { otherwise. }
\end{aligned}
$$

Some additional remarks about the process to understand:
- The probability to make the move $\alpha(x,y)$ is calculated after drawing a from the candidate density q(x,y)
- In practice this probability to move is simulated by simply comparing a draw from a uniform distribution with the vaue of $\alpha(x,y)$. If it is inferior to the draw, we make the move. 
- If the draw indicates that we do not do the move, the next sample is x (we stay at the same state). And then we can redraw from the candidate density and repeat.
- The choice of the candidate density could be arbitrary but some choices have advantages like choosing a symmetric distribution. In this case : q(x,y)=q(y,x). And so the probability of move $\alpha(x,y)$ is simply $\pi(y)/\pi(x)$.

```python
def metropolis(pi, q, draw_from_q, initial_sample, n):
    x = initial_sample
    res = [x]
    rejected = 0
    for i in range(n):
        y = draw_from_q(x)
        div = (pi(x)*q(x,y))
        if div == 0:
            alpha = 1
        else:
            alpha = min((pi(y)*q(y,x))/div,1)
        u = uniform.rvs()
        if u<=alpha:
            x = y
        else:
            rejected +=1
        res+= [x]
    print('rejected : {}'.format(rejected))
    acceptance_rate = 1-(rejected/n)
    print('acceptance_rate : {}'.format(acceptance_rate))
    return res,acceptance_rate
```

In this example, we are going to assume we know how to draw samples from an multivariate distribution with a diagonal covariance (independance of the covariates and thus it only consists in sampling from two normal distributions in 2D). 

```python
def pi(x):
    return multivariate_normal.pdf(x, mean=[1,2], cov=[[1,0.9],[0.9,1]])
    
def q(x,y):
    return multivariate_normal.pdf(x-y, mean=[0,0], cov=[[1,0],[0,1]])

def draw_from_q(x):
    return x+multivariate_normal.rvs(mean=[0,0], cov=[[1,0],[0,1]])
```

```python
t = time.time()
res,acceptance_rate = metropolis(pi, q, draw_from_q, [0,0],50000)
time.time()-t
```
    17.157570123672485

```python
acceptance_rate 
```
    0.3144

A good acceptance rate is typically 23%, if it is too high, it means that the chain could me more efficient.
If it is too low, you should increase the variance of your transition kernel to explore the space. 

```python
plt.plot(res[:,0])
```

![png]({{ site.baseurl }}/assets/20230110/MCMC/MCMC_87_1.png){: .center-image }

```python
res = np.array(res)
plot = plt.hist2d(res[:,0],res[:,1],bins=50)
```

![png]({{ site.baseurl }}/assets/20230110/MCMC/MCMC_88_0.png){: .center-image }

To realize what were the advantages of MCMC, let's compare with the same algorithm but using A-R. We are going to assume, like for the MCMC,that we can draw samples from an multivarite with independant variates. The challenge here, that we did not have to care about with MCMC is to find the dominance constant c. It is not straightforward to find it. We should find the maximum of the ratio of two multivariate laws. It could be possible but we will not do it here. 

## 4. MCMC for Bayesian Inference

Like said in the part on bayesian inference. We need a sampling strategy because computing the posterior is not feasible in the case of the simultaneous estimation of several parameters. A-R is not adapted to this case because it is hard to find the dominance of a posterior distribution whose expression is very complex (the calculus of the likelihood results in a product that is the size of the data). 

### On the example of heights

This example has been applied naively without MCMC in the part that concerns bayesian inference. The candidate transition kernel has to be two dimensional, so will choose a multivariate with variance (5,1) truncated for the sigma to avoid sampling negative sigmas and we will begin our walk at (100, 11). Note that it is a bad start and we will have to let our MCMC converge.  

```python
# We are going to use a truncated normal law to transition for sigma because sigma cannot be less than 2 and more than 12.
# This is used only for sigma as sigma must be positive
# We could use a truncated transition for the mean as well but we are lazy. 

for i in range(5):
    print(truncnorm.rvs(loc=11, scale=2,a=a,b=b))
```

    8.238154586056801
    10.559714481855604
    9.03186866058273
    9.033027377668212
    11.160100319065744

```python
data_h = np.array([183, 168, 177, 170, 175, 177, 178, 166, 174, 178])

def get_posterior(theta, data_h, prior, likelihood):
    posterior = prior(theta)
    likelihoods = likelihood(data_h, theta)
    for i in range(len(data_h)):
        posterior = likelihoods[i]*posterior
    return posterior
    
def prior2(theta):
    return norm.pdf(theta[0],loc=165,scale=14)*uniform.pdf(theta[1],loc=2,scale=12)

def likelihood2(h, theta):
    return norm.pdf(h,loc=theta[0],scale=theta[1])

def pi2(theta):
    return get_posterior(theta, data_h, prior2, likelihood2)


def get_bounds(clip_a, clip_b, mean, std):
    return (clip_a - mean) / std, (clip_b - mean) / std

def q2(theta1,theta2):
    a,b=get_bounds(2,12,theta1[1],1)
    return norm.pdf(theta1[0]-theta2[0], loc=0, scale=5)*truncnorm.pdf(theta2[1], loc=theta1[1], scale=1,a=a,b=b)

def draw_from_q2(theta):
    theta0 = theta[0] + norm.rvs(loc=0, scale=5)
    a,b=get_bounds(2,12,theta[1],1)
    theta1 = truncnorm.rvs(loc=theta[1], scale=1,a=a,b=b) 
    return np.array([theta0, theta1])

t = time.time()
res2, ar2 = metropolis(pi2, q2, draw_from_q2, np.array([100,11]),10000)
time.time()-t
```
    rejected : 6448
    acceptance_rate : 0.35519999999999996

    7.543943643569946


```python
ar2
```
    0.35519999999999996


```python
res2 = np.array(res2)
plt.plot(res2[:,0])
```

![png]({{ site.baseurl }}/assets/20230110/MCMC/MCMC_94_1.png){: .center-image }

We can see that the chains seems to stabilize at 1000. We can burn the first 1000 samples.

```python
plot = plt.hist2d(res2[1000:,0],res2[1000:,1],bins=40)
```

![png]({{ site.baseurl }}/assets/20230110/MCMC/MCMC_96_0.png){: .center-image }

```python
print('mu_mean = {}'.format(np.mean(res2[1000:,0])))
print('sigma_mean = {}'.format(np.mean(res2[1000:,1])))
```

    mu_mean = 174.37747908756984
    sigma_mean = 6.166015526950849

The advantage over the evaluation of the posterior is not clear here. What we can see is that the sampling does not make necessary the choice of the resolution and the size of the grid a variable, we only have to choose how much samples to burn. The advantage is clear in the case for higher dimensions and in cases where the choice of resolutions and size of the grid is difficult. 

Two notes : 
- In this case the choice to evaluate the values of the posterior (grid method) is feasible and thus it has the advantage to yield the true values of the posterior directly (values that are proportional to the true posterior). The MCMC only converges to the distribution. 
- But an advantage of MCMC is that if we want to have a better representation, we only have to make more samples whereas with the evaluation of the posterior we would have calculate everything from the begining or make a grid pattern exploration that allows an incremental augmentation of the resolution. 

#### Influence of the kernel of transition

We are going to choose a worst transition kernel (with more rejections) to see the effect. We are going to choose uniform transitions. As a remark, this also shows that a transition kernel q(x,y) could choosen so that it is independent from x : q(x,y)=h(y). The main difference will be the acceptance rate. A good acceptance rate is typically 23%, if it is too high, it means that the chain could me more efficient.
If it is too low, you should increase the variance of your transition kernel to explore the space. 

In our case, the acceptance rate it too low and thus many samples are rejected (in fact you stay at the same place). It means that we are not going to explore the space as rapidly as we could hope for. In both case, a low or high acceptance rate, it seems that it does not favor a good exploration of the space. 

```python
data_h = np.array([183, 168, 177, 170, 175, 177, 178, 166, 174, 178])

def get_posterior(theta, data_h, prior, likelihood):
    posterior = prior(theta)
    likelihoods = likelihood(data_h, theta)
    for i in range(len(data_h)):
        posterior = likelihoods[i]*posterior
    return posterior
    
def prior2(theta):
    return norm.pdf(theta[0],loc=165,scale=14)*uniform.pdf(theta[1],loc=2,scale=12)

def likelihood2(h, theta):
    return norm.pdf(h,loc=theta[0],scale=theta[1])

def pi2(theta):
    return get_posterior(theta, data_h, prior2, likelihood2)


def get_bounds(clip_a, clip_b, mean, std):
    return (clip_a - mean) / std, (clip_b - mean) / std

def q2(theta1,theta2):
    return prior2(theta2)

def draw_from_q2(theta):
    theta0 = norm.rvs(loc=165,scale=14)
    theta1 = uniform.rvs(loc=2,scale=12)
    return np.array([theta0, theta1])

t = time.time()
res2, ar2 = metropolis(pi2, q2, draw_from_q2, np.array([100,11]),10000)
time.time()-t
```
    rejected : 9403
    acceptance_rate : 0.059699999999999975

    6.104666233062744

```python
ar2
```
    0.059699999999999975

```python
res2 = np.array(res2)
plt.plot(res2[:,1])
```

![png]({{ site.baseurl }}/assets/20230110/MCMC/MCMC_102_1.png){: .center-image }

```python
plot = plt.hist2d(res2[1000:,0],res2[1000:,1],bins=40)
```

![png]({{ site.baseurl }}/assets/20230110/MCMC/MCMC_103_0.png){: .center-image }

```python
print('mu_mean = {}'.format(np.mean(res2[1000:,0])))
print('sigma_mean = {}'.format(np.mean(res2[1000:,1])))
```
    mu_mean = 174.59163450346975
    sigma_mean = 5.868186782422386

### On another a more complex example

We have observations of the model : $y_{t}=\phi_{1} y_{t-1}+\phi_{2} y_{t-2}+\epsilon_{t}$. Where 
- $\phi_{1}=1$
- $\phi_{2}=-0.5$
- $\epsilon_{t} \sim N(0,\sigma)$ with $\sigma=1$

The goal is to find the values of $\theta = (\sigma,\phi_1, \phi_2)$ that are supposed to be unknown. 

The process is stationnary (see courses about auto-regressive processes), it means that the expected value and the covariance are not time depedent.
In stationnary conditions we have that $(y1,y2)\sim N(0,V(\theta))$. Then for the rest of the $y_i$, the follow a normal law with mean $\phi_{1} y_{t-1}+\phi_{2} y_{t-2}$ and variance $\sigma$.

We have the likelihood : $P(y \mid \theta) = \prod_{i=3}^{i=n} P(y_i \mid \theta, [y_1,y_{i-1}]) \times P(y1,y2 \mid \theta)=\prod_{i=3}^{i=n} P(y_i \mid \theta, y_{i-2},y_{i-1}) \times P(y1,y2 \mid \theta)$ because $y_{i}$ does not depends on $y_k$ with k<i-2. 

But to simplify we are going to consider that we know y1 and y2 and that we do not necessarly start when the system is stable. We are going to fix y1=1 and y2=0. And so our likelihood function will simply be : $P(y \mid \theta) = \prod_{i=3}^{i=n} P(y_i \mid \theta, y_{i-2},y_{i-1}) $. We still know that we have stationnarity conditions respected so, a classical resutls for autoregressive models is that :

$$
\phi_{1}+\phi_{2}<1 ; \quad-\phi_{1}+\phi_{2}<1 ; \quad \phi_{2}>-1
$$

That gives the conditions $\phi_{1} \in [-2;2]$ and  $\phi_{2} \in [-1;1]$

We are going to choose as priors : 
- U[0,10] for $\sigma$
- $\phi_1$ and $\phi_2$ are sampled uniformally on the set of possible values that respects the stationnarity conditions. 

We are going to choose as candidate transition kernels truncated random walks on [0,10] , [-2,2] and [-1,1]. This may not be the best candidate kernels.

```python
# Let's generate the observations

def generate_observations(n=100, y0=1, y1=1):
    res = [y0,y1]
    yt1 = y1
    yt2 = y0
    for i in range(n):
        noise = norm.rvs(scale=1)
        yt = yt1-0.5*yt2+noise
        res += [yt]
        yt2 = yt1
        yt1 = yt
    return res
```

```python
obs = generate_observations()
plt.plot(obs)
```
![png]({{ site.baseurl }}/assets/20230110/MCMC/MCMC_109_1.png){: .center-image }

The likelihood

```python
def get_posterior(theta, data_h, prior, likelihood):
    posterior = prior(theta)
    likelihood = likelihood(data_h, theta)
    return posterior*likelihood
    
def prior3(theta):
    if theta[1] + theta[2]>=1:
        return 0
    elif theta[2] - theta[1]>=1:
        return 0
    elif theta[2]<=-1:
        return 0
    else:
        return 1 

def likelihood3(h, theta):
    l = len(h)
    h = [1,0]+h
    res = 1
    for i in range(l):
        mean = theta[1]*h[i+1]+theta[2]*h[i]
        res*=norm.pdf(h[i+2],loc=mean,scale=theta[0])
    return res

def pi3(theta):
    return get_posterior(theta, obs, prior3, likelihood3)

def get_bounds(clip_a, clip_b, mean, std):
    return (clip_a - mean) / std, (clip_b - mean) / std

a_l = np.array([0,-2,-1])
b_l = np.array([10,2,1])
scales = np.array([0.25,0.1,0.1])

def q3(theta1,theta2):
    a,b = get_bounds(a_l, b_l, theta1, scales)
    p0 = truncnorm.pdf(theta2[0],loc=theta1[0], scale=scales[0],a=a[0],b=b[0])
    p1 = truncnorm.pdf(theta2[1],loc=theta1[1], scale=scales[1],a=a[1],b=b[1])
    p2 = truncnorm.pdf(theta2[2],loc=theta1[2], scale=scales[2],a=a[2],b=b[2])
    return p0*p1*p2

def draw_from_q3(theta):
    a,b = get_bounds(a_l, b_l, theta, scales)
    theta0 = truncnorm.rvs(loc=theta[0], scale=scales[0],a=a[0],b=b[0])
    theta1 = truncnorm.rvs(loc=theta[1], scale=scales[1],a=a[1],b=b[1])
    theta2 = truncnorm.rvs(loc=theta[2], scale=scales[2],a=a[2],b=b[2])
    return np.array([theta0,theta1,theta2])

t = time.time()
res3, ar3 = metropolis(pi3, q3, draw_from_q3, np.array([5,0,0]),10000)
time.time()-t
```
    rejected : 8234
    acceptance_rate : 0.17659999999999998
    125.5704653263092


```python
res3 = np.array(res3)
plt.plot(res3[:,0])
```

![png]({{ site.baseurl }}/assets/20230110/MCMC/MCMC_112_1.png){: .center-image }

```python
plot = plt.hist2d(res3[250:,0],res3[250:,1],bins=40)
```

![png]({{ site.baseurl }}/assets/20230110/MCMC/MCMC_113_0.png){: .center-image }

```python
plot = plt.hist2d(res3[250:,0],res3[250:,2],bins=40)
```

![png]({{ site.baseurl }}/assets/20230110/MCMC/MCMC_114_0.png){: .center-image }

```python
print('sigma_mean = {}'.format(np.mean(res3[250:,0])))
print('phi1_mean = {}'.format(np.mean(res3[250:,1])))
print('phi2_mean = {}'.format(np.mean(res3[250:,2])))
```

    sigma_mean = 0.9925867051366783
    phi1_mean = 0.9353596745941084
    phi2_mean = -0.4289829676081113

```python
print('sigma_std = {}'.format(np.std(res3[250:,0])))
print('phi1_std = {}'.format(np.std(res3[250:,1])))
print('phi2_std = {}'.format(np.std(res3[250:,2])))
```

    sigma_std = 0.07061722727497839
    phi1_std = 0.09657201146467626
    phi2_std = 0.09573109510333318

# C. More on MCMC techniques

- Cool animations in [this site](https://chi-feng.github.io/mcmc-demo/app.html) [^chi-feng] show in a very intuitive way how MCMC and its variants works. 
- Gibbs algorithm is a special case of the Metropolis-Hastings algorithm according to Chib and Greenberg [^chib-greenberg]. Casella and George [^casella-georges] is also a reference to understand Gibb's variant. 

## Some popular libraries for MCMC

Here are some popular libraries that I know of to do MCMC, this may not be a complete list but i believe thse are serious options : 

- [Stan](https://mc-stan.org/)  (C++)
- [PyStan](https://github.com/stan-dev/pystan ) (Python) 
- [PyMC3](https://docs.pymc.io/en/v3/index.html) (Python) 
- [Turing](https://turing.ml/stable/) (Julia)


# References

[^vetagro]: M. L. Delignette-Muller, « Introduction to bayesian inference ».
[^datapythonista]: <https://datapythonista.me/blog/bayesian-inference-tutorial-a-hello-world-example.html>
[^wikidensity]: <https://en.wikipedia.org/wiki/Probability_density_function>
[^wikimarkov]: <https://en.wikipedia.org/wiki/Markov_chain>
[^chib-greenberg]: S. Chib et E. Greenberg, « Understanding the Metropolis-Hastings Algorithm », The American Statistician, vol. 49, nᵒ 4, p. 327‑335, 1995, doi: 10.2307/2684568.
[^chi-feng]: <https://chi-feng.github.io/mcmc-demo/app.html>
[^casella-georges]: G. Casella et E. I. George, « Explaining the Gibbs Sampler », The American Statistician, vol. 46, nᵒ 3, p. 167‑174, 1992, doi: 10.2307/2685208.




